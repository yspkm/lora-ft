{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GenerationConfig\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "sys.path.append(os.path.join(os.getcwd(), \"peft/src/\"))\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def get_huggingface_token(config_file_path:str='config.yaml'):\n",
    "    import yaml\n",
    "    token = None\n",
    "    with open(config_file_path, \"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "        token = config[\"huggingface\"][\"token\"]\n",
    "    return token\n",
    "\n",
    "#model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "#lora_weights = \"yspkm/Meta-Llama-3-8B-Instruct-lora-math\"\n",
    "#model_name = 'google/gemma-2-9b-it'\n",
    "#model_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "#lora_weights = \"yspkm/Mistral-7B-Instruct-v0.3-lora-math\"\n",
    "model_name = 'google/gemma-2b-it'\n",
    "lora_weights = \"yspkm/gemma-2b-it-lora-math\"\n",
    "#lora_weights = 'trained_models/mistral/math'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=get_huggingface_token()\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "#peft_model = torch.compile(peft_model)\n",
    "\n",
    "#base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", device_map='auto')\n",
    "\n",
    "#config = PeftConfig.from_pretrained(\"yspkm/Mistral-7B-Instruct-v0.3-lora-math\")\n",
    "#model = PeftModel.from_pretrained(base_model, config)\n",
    "#model = PeftModel.from_pretrained(base_model, \"yspkm/Mistral-7B-Instruct-v0.3-lora-math\")\n",
    "\n",
    "'''\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    do_sample=True,\n",
    "    num_beams=5)\n",
    "'''\n",
    "#text_generator = pipeline('text-generation', model=peft_model, tokenizer=tokenizer, max_new_tokens=256, device_map='auto', config=generation_config)\n",
    "text_generator = pipeline('text-generation', model=peft_model, tokenizer=tokenizer, max_new_tokens=256, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"Please choose the correct solution to the question: dresser\\n\\nSolution1: replace drawer with bobby pin \\n\\nSolution2: finish, woodgrain with  bobby pin \\n\\nAnswer format: solution1/solution2\"\n",
    "#instruction = \"Jack received 9 emails in the morning, 10 emails in the afternoon and 7 emails in the evening. How many more emails did Jack receive in the morning than in the evening?\"\n",
    "instruction = \"Zach wants to ride the Ferris wheel , the roller coaster , and the log ride . The Ferris wheel costs 2 tickets , the roller coaster costs 7 tickets and the log ride costs 1 ticket . Zach has 1 ticket . How many more tickets should Zach buy ?\"\n",
    "#instruction = \"Paige had 27 files on her computer. She deleted 9 of them and put the rest into folders with 6 files in each one. How many folders did Paige end up with?\"\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  \n",
    "\n",
    "                ### Instruction:\n",
    "                {instruction}\n",
    "                \n",
    "                ### Response:\"\"\" \n",
    "generated_text = text_generator(prompt)[0]['generated_text']\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"peft/src/\"))\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_name = 'google/gemma-2-9b-it'\n",
    "lora_weights = \"yspkm/gemma-2-9b-it-lora-math\"\n",
    "def get_huggingface_token(config_file_path:str='config.yaml'):\n",
    "    import yaml\n",
    "    token = None\n",
    "    with open(config_file_path, \"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "        token = config[\"huggingface\"][\"token\"]\n",
    "    return token\n",
    "\n",
    "HF_TOKEN = get_huggingface_token()\n",
    "\n",
    "import re\n",
    "\n",
    "# 모델 이름 설정 (예: bert-base-uncased)\n",
    "\n",
    "# Hugging Face 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    "    #merge_weights=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "#peft_model.eval()\n",
    "#peft_model = peft_model.compile()\n",
    "#text_generator = pipeline('text-generation', model=peft_model, tokenizer=tokenizer, max_new_tokens=256, device_map='auto')\n",
    "def get_prompt(instruction:str):\n",
    "    return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  \n",
    "\n",
    "                ### Instruction:\n",
    "                {instruction}\n",
    "                \n",
    "                ### Response:\"\"\" \n",
    "\n",
    "def generate_text(instruction: str, text_generator):\n",
    "    prompt = get_prompt(instruction)\n",
    "    print(f\"Prompt: {prompt}\\n\\n\")\n",
    "    generated_text = text_generator(prompt)[0]['generated_text']\n",
    "    print(generated_text)\n",
    "    return prompt, generated_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(peft_model, depth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Zach wants to ride the Ferris wheel , the roller coaster , and the log ride . The Ferris wheel costs 2 tickets , the roller coaster costs 7 tickets and the log ride costs 1 ticket . Zach has 1 ticket . How many more tickets should Zach buy ?\"\n",
    "%time generate_text(instruction, text_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
